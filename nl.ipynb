{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:34:11.141069Z",
     "start_time": "2025-04-18T07:34:11.007819Z"
    }
   },
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:34:35.488827Z",
     "start_time": "2025-04-18T07:34:32.200280Z"
    }
   },
   "source": "raw_data = pd.read_csv('NIKL_NEWSPAPER_2023_CSV/NEWSPAPER_2022_1.csv')",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects only the sentence column and makes a new DataFrame df with one column: each row is a Korean sentence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:34:36.165387Z",
     "start_time": "2025-04-18T07:34:36.153496Z"
    }
   },
   "source": [
    "df = pd.DataFrame(raw_data['sentence'])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls generate_labels (from preprocess.py) to:\n",
    "\n",
    "1) Normalize and remove spaces from each sentence,\n",
    "\n",
    "2) Produce a binary label sequence marking where spaces originally were,\n",
    "\n",
    "3) Collect the set of all characters seen (chars),\n",
    "\n",
    "4) Return the augmented DataFrame (with unspaced and labels columns) and the character set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:34:57.358955Z",
     "start_time": "2025-04-18T07:34:37.273714Z"
    }
   },
   "source": [
    "import preprocess\n",
    "\n",
    "df, chars = preprocess.generate_labels(df)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:35:45.259030Z",
     "start_time": "2025-04-18T07:35:45.250543Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            sentence  \\\n",
       "0                     [영상]“위기를 이겨내고 일상으로” 부산 기관장 신년사   \n",
       "1  취임 2년 차를 맞는 박형준 부산시장은 코로나19 위기를 이겨내고 있는 부산시민들에...   \n",
       "2  박 시장은 “코로나19는 번번이 우리의 희망의 길목을 막아섰고, 지금도 여전히 민생...   \n",
       "3  그러면서 “숱한 어려움을 헤쳐온 위대한 시민의 힘으로 2022년을 코로나를 극복하고...   \n",
       "4  박 시장은 “치밀한 전략으로 2030세계박람회 유치를 위한 국제박람회기구 현지 실사...   \n",
       "\n",
       "                                            unspaced  \\\n",
       "0                          [영상]\"위기를이겨내고일상으로\"부산기관장신년사   \n",
       "1  취임2년차를맞는박형준부산시장은코로나19위기를이겨내고있는부산시민들에게감사의인사를먼저전했다.   \n",
       "2  박시장은\"코로나19는번번이우리의희망의길목을막아섰고,지금도여전히민생을위협하고있다\"며\"...   \n",
       "3  그러면서\"숱한어려움을헤쳐온위대한시민의힘으로2022년을코로나를극복하고일상을회복하는해,...   \n",
       "4  박시장은\"치밀한전략으로2030세계박람회유치를위한국제박람회기구현지실사를성공적으로이끌고...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "2  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...  \n",
       "3  [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...  \n",
       "4  [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>unspaced</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[영상]“위기를 이겨내고 일상으로” 부산 기관장 신년사</td>\n",
       "      <td>[영상]\"위기를이겨내고일상으로\"부산기관장신년사</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>취임 2년 차를 맞는 박형준 부산시장은 코로나19 위기를 이겨내고 있는 부산시민들에...</td>\n",
       "      <td>취임2년차를맞는박형준부산시장은코로나19위기를이겨내고있는부산시민들에게감사의인사를먼저전했다.</td>\n",
       "      <td>[0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>박 시장은 “코로나19는 번번이 우리의 희망의 길목을 막아섰고, 지금도 여전히 민생...</td>\n",
       "      <td>박시장은\"코로나19는번번이우리의희망의길목을막아섰고,지금도여전히민생을위협하고있다\"며\"...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그러면서 “숱한 어려움을 헤쳐온 위대한 시민의 힘으로 2022년을 코로나를 극복하고...</td>\n",
       "      <td>그러면서\"숱한어려움을헤쳐온위대한시민의힘으로2022년을코로나를극복하고일상을회복하는해,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>박 시장은 “치밀한 전략으로 2030세계박람회 유치를 위한 국제박람회기구 현지 실사...</td>\n",
       "      <td>박시장은\"치밀한전략으로2030세계박람회유치를위한국제박람회기구현지실사를성공적으로이끌고...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds two mappings from your character set:\n",
    "\n",
    "1) char2idx: maps each character (plus a special <PAD> token) to a unique integer index.\n",
    "\n",
    "2) idx2char: the inverse lookup (list of characters by index).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:35:46.959476Z",
     "start_time": "2025-04-18T07:35:46.956577Z"
    }
   },
   "source": [
    "char2idx, idx2char = preprocess.generate_mappings(chars)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts each row of df into PyTorch tensors:\n",
    "\n",
    "1) input_tensor: list of 1D LongTensors where each element is the index of a character in the unspaced text.\n",
    "\n",
    "2) label_tensor: list of 1D LongTensors of the same length, with 1s where a space should follow and 0s otherwise."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:36:20.412098Z",
     "start_time": "2025-04-18T07:35:48.005585Z"
    }
   },
   "source": [
    "input_tensor, label_tensor = preprocess.generate_tensors(df, char2idx)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Set up for Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:36:38.983764Z",
     "start_time": "2025-04-18T07:36:38.794190Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from lstm_model import KoreanSpacingLSTM\n",
    "from cnn_model import KoreanSpacingCNN\n",
    "from transformer_model import KoreanSpacingTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:36:40.409060Z",
     "start_time": "2025-04-18T07:36:40.377519Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class SpacingDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs    \n",
    "        self.labels = labels    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs, Ys = zip(*batch)\n",
    "    Xp = pad_sequence(Xs, batch_first=True, padding_value=0)\n",
    "    Yp = pad_sequence(Ys, batch_first=True, padding_value=-100)\n",
    "    return Xp.to(device), Yp.to(device)\n",
    "\n",
    "# build and split\n",
    "full_ds = SpacingDataset(input_tensor, label_tensor)\n",
    "n = len(full_ds)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    full_ds, [n_train, n_val, n_test],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_ds,  batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,    batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 800000/100000/100000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:36:42.822234Z",
     "start_time": "2025-04-18T07:36:42.766856Z"
    }
   },
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for X, Y in loader:\n",
    "        logits = model(X)                          \n",
    "        B, S, C = logits.shape\n",
    "        loss = criterion(logits.view(-1, C), Y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / len(loader)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in loader:\n",
    "            logits = model(X)\n",
    "            B, S, C = logits.shape\n",
    "            total += criterion(logits.view(-1, C), Y.view(-1)).item()\n",
    "    return total / len(loader)\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # —— Train epoch with live batch‐loss reporting ——\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for X, Y in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)  # (B, S, C)\n",
    "            B, S, C = logits.shape\n",
    "            loss = criterion(logits.reshape(-1, C), Y.reshape(-1)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(batch_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_train = running_loss / len(train_loader)\n",
    "\n",
    "        # —— Validation loss ——\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_loader:\n",
    "                logits = model(X)\n",
    "                B, S, C = logits.shape\n",
    "                val_loss += criterion(logits.reshape(-1, C), Y.reshape(-1)).item() \n",
    "        avg_val = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} — \"\n",
    "              f\"train_loss: {avg_train:.4f}  \"\n",
    "              f\"val_loss: {avg_val:.4f}  \"\n",
    "              f\"time: {epoch_time:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Hyperparameter Tuning and Training"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:19:45.625136Z",
     "start_time": "2025-04-18T08:10:59.152340Z"
    }
   },
   "source": [
    "lstm = KoreanSpacingLSTM(\n",
    "    vocab_size=len(char2idx),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "fit(lstm, train_loader, val_loader, epochs=10, lr=1e-3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73098733816549598540c9cb83d5a4ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — train_loss: 0.1101  val_loss: 0.0756  time: 48.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acc429248d49497aacf9e3b4afb38abc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 — train_loss: 0.0757  val_loss: 0.0670  time: 50.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8792e5df7184b23a19d80d6164b6c4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 — train_loss: 0.0690  val_loss: 0.0632  time: 49.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8506787fa4db4aeba1fae957ae86baee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 — train_loss: 0.0659  val_loss: 0.0618  time: 47.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3de531711fae4eb891ebc57bfae16c02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 — train_loss: 0.0637  val_loss: 0.0598  time: 49.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94672efad4cc4a6aa6e6d9fa06fa8bb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 — train_loss: 0.0618  val_loss: 0.0588  time: 51.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "810791f27d4c4412ac36b536e1b7fbb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 — train_loss: 0.0605  val_loss: 0.0578  time: 50.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bd801ce3b764f93a7edfb9d2b54421e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 — train_loss: 0.0594  val_loss: 0.0572  time: 50.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cb5d3544d354376a616142f419907e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 — train_loss: 0.0585  val_loss: 0.0567  time: 50.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3475837320724deeb67735e986c6ceea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 — train_loss: 0.0578  val_loss: 0.0561  time: 50.7s\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:39:51.652283Z",
     "start_time": "2025-04-18T08:35:09.616298Z"
    }
   },
   "source": [
    "cnn = KoreanSpacingCNN(\n",
    "    vocab_size=len(char2idx),\n",
    "    embedding_dim=128,\n",
    "    num_filters=128,\n",
    "    kernel_sizes=[3,5],\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "fit(cnn, train_loader, val_loader, epochs=10, lr=3e-3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b048d6bea6a4ac4ba9df9fc21011c85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — train_loss: 0.1077  val_loss: 0.0826  time: 26.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b03fefc1c096418aa4a53e187a20840b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 — train_loss: 0.0831  val_loss: 0.0760  time: 26.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d17e53180d194007b7079ec799b30176"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 — train_loss: 0.0780  val_loss: 0.0729  time: 27.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed77a4a3abd047d19f0711fd5af564f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 — train_loss: 0.0753  val_loss: 0.0709  time: 26.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca57166fa4f3487cadb018ab74acc400"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 — train_loss: 0.0735  val_loss: 0.0696  time: 27.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "464be495cf6348a1a8c5d45a7151a10f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 — train_loss: 0.0722  val_loss: 0.0688  time: 26.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9dfe4cb386e40a28668932a70c7e974"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 — train_loss: 0.0712  val_loss: 0.0683  time: 26.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd8b134cc9bf4187a79d5dd3ae955447"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 — train_loss: 0.0705  val_loss: 0.0678  time: 26.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69e9b3799f5b41a2857b887253296036"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 — train_loss: 0.0699  val_loss: 0.0671  time: 26.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d723b192ca694d0880d153a68c741699"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 — train_loss: 0.0693  val_loss: 0.0669  time: 27.7s\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transformer = KoreanSpacingTransformer(\n",
    "    vocab_size=len(char2idx),\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=1000,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "fit(transformer, train_loader, val_loader, epochs=3, lr=1e-3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for testing"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T07:41:36.043110Z",
     "start_time": "2025-04-18T07:41:35.149524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    total_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_loader:\n",
    "            logits = model(X)               \n",
    "            B, S, C = logits.shape\n",
    "\n",
    "            # 1a) accumulate loss\n",
    "            loss = criterion(logits.reshape(-1, C), Y.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 1b) compute spacing‐accuracy\n",
    "            preds = logits.argmax(-1)      \n",
    "            mask  = (Y != -100)            # ignore padded positions\n",
    "            correct += (preds[mask] == Y[mask]).sum().item()\n",
    "            total   += mask.sum().item()\n",
    "\n",
    "            all_preds.extend(preds[mask].cpu().numpy())\n",
    "            all_labels.extend(Y[mask].cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f\"Test  — loss: {avg_loss:.4f}, accuracy: {accuracy:.4%}\")\n",
    "    print(f\"precision: {precision:.4f}, recall: {recall:.4f}, \"\n",
    "          f\"F1 score: {f1:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:20:26.887652Z",
     "start_time": "2025-04-18T08:20:13.113160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Evaluating LSTM on test set:\")\n",
    "test_model(lstm, test_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LSTM on test set:\n",
      "Test  — loss: 0.0567, accuracy: 97.7645%\n",
      "precision: 0.9618, recall: 0.9563, F1 score: 0.9590\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:40:37.239298Z",
     "start_time": "2025-04-18T08:40:24.764853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Evaluating CNN on test set:\")\n",
    "test_model(cnn, test_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CNN on test set:\n",
      "Test  — loss: 0.0672, accuracy: 97.3414%\n",
      "precision: 0.9535, recall: 0.9491, F1 score: 0.9513\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:32:13.549554Z",
     "start_time": "2025-04-18T09:31:45.858661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Evaluating Transformer on test set:\")\n",
    "test_model(transformer, test_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Transformer on test set:\n",
      "Test  — loss: 0.3102, accuracy: 86.5673%\n",
      "precision: 0.8177, recall: 0.6549, F1 score: 0.7273\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save Model + Mapping"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:26:45.279652Z",
     "start_time": "2025-04-18T08:26:45.270332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "    'lstm_state_dict': lstm.state_dict(),\n",
    "    'char2idx':        char2idx,\n",
    "    'idx2char':        idx2char\n",
    "}, 'spacing_model_lstm.pt')\n",
    "print(\"Saved LSTM weight and mapping to spacing_model_lstm.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LSTM weight and mapping to spacing_model_lstm.pt\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T08:40:42.208224Z",
     "start_time": "2025-04-18T08:40:42.200367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "    'cnn_state_dict':  cnn.state_dict(),\n",
    "    'char2idx':        char2idx,\n",
    "    'idx2char':        idx2char\n",
    "}, 'spacing_model_cnn.pt')\n",
    "print(\"Saved CNN weight and mapping to spacing_model_cnn.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CNN weight and mapping to spacing_model_cnn.pt\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:32:22.311985Z",
     "start_time": "2025-04-18T09:32:22.277278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "    'transformer_state_dict':  transformer.state_dict(),\n",
    "    'char2idx':        char2idx,\n",
    "    'idx2char':        idx2char\n",
    "}, 'spacing_model_transformer.pt')\n",
    "print(\"Saved transformer weight and mapping to spacing_model_transformer.pt\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer weight and mapping to spacing_model_transformer.pt\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load Saved Models and Make Predictions on Text"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from lstm_model import KoreanSpacingLSTM\n",
    "from cnn_model import KoreanSpacingCNN\n",
    "from transformer_model import KoreanSpacingTransformer\n",
    "\n",
    "lstm_checkpoint = torch.load('spacing_model_lstm.pt')\n",
    "lstm = KoreanSpacingLSTM(\n",
    "    vocab_size=len(lstm_checkpoint['char2idx']),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "lstm.load_state_dict(lstm_checkpoint['lstm_state_dict'])\n",
    "lstm.eval()\n",
    "\n",
    "cnn_checkpoint = torch.load('spacing_model_cnn.pt')\n",
    "cnn = KoreanSpacingCNN(\n",
    "    vocab_size=len(cnn_checkpoint['char2idx']),\n",
    "    embedding_dim=128,\n",
    "    num_filters=128,\n",
    "    kernel_sizes=[3,5],\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "cnn.load_state_dict(cnn_checkpoint['cnn_state_dict'])\n",
    "cnn.eval()\n",
    "\n",
    "transformer_checkpoint = torch.load('spacing_model_transformer.pt')\n",
    "transformer = KoreanSpacingTransformer(\n",
    "    vocab_size=len(transformer_checkpoint['char2idx']),\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=1000,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "transformer.load_state_dict(transformer_checkpoint['transformer_state_dict'])\n",
    "transformer.eval()\n",
    "\n",
    "char2idx = lstm_checkpoint['char2idx']\n",
    "idx2char = lstm_checkpoint['idx2char']\n",
    "\n",
    "print(\"Loaded all models.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T10:01:22.185155Z",
     "start_time": "2025-04-18T10:01:22.182789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(model, text: str) -> str:\n",
    "    indices = [char2idx.get(character, 0) for character in text]\n",
    "    input_ = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_)\n",
    "        predictions = logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "    prediction = ''\n",
    "    for i, character in enumerate(text):\n",
    "        prediction += character\n",
    "        if predictions[i] == 1:\n",
    "            prediction += ' '\n",
    "    return prediction"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T10:01:23.418334Z",
     "start_time": "2025-04-18T10:01:23.403653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = '이문장은띄어쓰기가없는문장입니다.'\n",
    "\n",
    "lstm_prediction = predict(lstm, text)\n",
    "cnn_prediction = predict(cnn, text)\n",
    "transformer_prediction = predict(transformer, text)\n",
    "\n",
    "print('Korean text without spaces:', text)\n",
    "print('LSTM model prediction:', lstm_prediction)\n",
    "print('CNN model prediction:', cnn_prediction)\n",
    "print('Transformer model prediction:', transformer_prediction)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean text without spaces: 이문장은띄어쓰기가없는문장입니다.\n",
      "LSTM model prediction: 이 문장은 띄어 쓰기가 없는 문장입니다.\n",
      "CNN model prediction: 이 문장은 띄어쓰기가 없는 문장입니다.\n",
      "Transformer model prediction: 이 문장은 띄어쓰기가 없는 문장입니다. \n"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_a5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
