{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('NIKL_NEWSPAPER_2023_CSV/NEWSPAPER_2022_1.csv')\n",
    "\n",
    "# TODO: option to read all csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects only the sentence column and makes a new DataFrame df with one column: each row is a Korean sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls generate_labels (from preprocess.py) to:\n",
    "\n",
    "1) Normalize and remove spaces from each sentence,\n",
    "\n",
    "2) Produce a binary label sequence marking where spaces originally were,\n",
    "\n",
    "3) Collect the set of all characters seen (chars),\n",
    "\n",
    "4) Return the augmented DataFrame (with unspaced and labels columns) and the character set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "\n",
    "df, chars = preprocess.generate_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds two mappings from your character set:\n",
    "\n",
    "1) char2idx: maps each character (plus a special <PAD> token) to a unique integer index.\n",
    "\n",
    "2) idx2char: the inverse lookup (list of characters by index).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, idx2char = preprocess.generate_mappings(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts each row of df into PyTorch tensors:\n",
    "\n",
    "1) input_tensor: list of 1D LongTensors where each element is the index of a character in the unspaced text.\n",
    "\n",
    "2) label_tensor: list of 1D LongTensors of the same length, with 1s where a space should follow and 0s otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, label_tensor = preprocess.generate_tensors(df, char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Set up for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from model        import KoreanSpacingSolver\n",
    "from cnn_model    import KoreanSpacingCNN\n",
    "from Transformer_model import KoreanSpacingTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class SpacingDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs    \n",
    "        self.labels = labels    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs, Ys = zip(*batch)\n",
    "    Xp = pad_sequence(Xs, batch_first=True, padding_value=0)\n",
    "    Yp = pad_sequence(Ys, batch_first=True, padding_value=-100)\n",
    "    return Xp.to(device), Yp.to(device)\n",
    "\n",
    "# build and split\n",
    "full_ds = SpacingDataset(input_tensor, label_tensor)\n",
    "n = len(full_ds)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    full_ds, [n_train, n_val, n_test],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_ds,  batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,    batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for X, Y in loader:\n",
    "        logits = model(X)                          \n",
    "        B, S, C = logits.shape\n",
    "        loss = criterion(logits.view(-1, C), Y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / len(loader)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in loader:\n",
    "            logits = model(X)\n",
    "            B, S, C = logits.shape\n",
    "            total += criterion(logits.view(-1, C), Y.view(-1)).item()\n",
    "    return total / len(loader)\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # —— Train epoch with live batch‐loss reporting ——\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for X, Y in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)  # (B, S, C)\n",
    "            B, S, C = logits.shape\n",
    "            loss = criterion(logits.reshape(-1, C), Y.reshape(-1)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(batch_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_train = running_loss / len(train_loader)\n",
    "\n",
    "        # —— Validation loss ——\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_loader:\n",
    "                logits = model(X)\n",
    "                B, S, C = logits.shape\n",
    "                val_loss += criterion(logits.reshape(-1, C), Y.reshape(-1)).item() \n",
    "        avg_val = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} — \"\n",
    "              f\"train_loss: {avg_train:.4f}  \"\n",
    "              f\"val_loss: {avg_val:.4f}  \"\n",
    "              f\"time: {epoch_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = KoreanSpacingSolver(\n",
    "    vocab_size=len(char2idx),\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=512\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = KoreanSpacingCNN(\n",
    "    vocab_size=len(char2idx),\n",
    "    embedding_dim=200,\n",
    "    num_filters=100,\n",
    "    kernel_sizes=[3,5],\n",
    "    dropout=0.2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = KoreanSpacingTransformer(\n",
    "    vocab_size=len(char2idx),\n",
    "    d_model=256,             \n",
    "    nhead=8,                 \n",
    "    num_encoder_layers=4,    \n",
    "    dim_feedforward=512,     \n",
    "    dropout=0.1,             \n",
    "    max_len=1000,             \n",
    "    num_labels=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "fit(lstm, train_loader, val_loader, epochs=5, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "fit(cnn, train_loader, val_loader, epochs=5, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer model\n",
    "fit(transformer, train_loader, val_loader, epochs=5, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    total_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_loader:\n",
    "            logits = model(X)               \n",
    "            B, S, C = logits.shape\n",
    "\n",
    "            # 1a) accumulate loss\n",
    "            loss = criterion(logits.reshape(-1, C), Y.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 1b) compute spacing‐accuracy\n",
    "            preds = logits.argmax(-1)      \n",
    "            mask  = (Y != -100)            # ignore padded positions\n",
    "            correct += (preds[mask] == Y[mask]).sum().item()\n",
    "            total   += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test  — loss: {avg_loss:.4f}, accuracy: {accuracy:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating LSTM on test set:\")\n",
    "test_model(lstm, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating CNN on test set:\")\n",
    "test_model(cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Transformer on test set:\")\n",
    "test_model(transformer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model + Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'lstm_state_dict': lstm.state_dict(),\n",
    "    'char2idx':        char2idx,\n",
    "    'idx2char':        idx2char\n",
    "}, 'spacing_models_lstm.pt')\n",
    "print(\"Saved LSTM weight and mapping to spacing_models_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'cnn_state_dict':  cnn.state_dict(),\n",
    "    'char2idx':        char2idx,\n",
    "    'idx2char':        idx2char\n",
    "}, 'spacing_models_cnn.pt')\n",
    "print(\"Saved CNN weight and mapping to spacing_models.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'transformer_state_dict':  transformer.state_dict(),\n",
    "    'char2idx':        char2idx,\n",
    "    'idx2char':        idx2char\n",
    "}, 'spacing_models_transformer.pt')\n",
    "print(\"Saved transformer weight and mapping to spacing_models_transformer.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_a5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
